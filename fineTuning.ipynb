{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb0090",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 1. Install dependencies\n",
    "# ================================\n",
    "!pip install -q transformers accelerate datasets peft bitsandbytes trl safetensors\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import itertools\n",
    "\n",
    "# ================================\n",
    "# 2. Load streaming datasets (RICO + MobileViews)\n",
    "# ================================\n",
    "# RICO Dataset - UI Screenshots and View Hierarchies\n",
    "rico_dataset1 = load_dataset(\n",
    "    path=\"shunk031/Rico\",\n",
    "    name=\"ui-screenshots-and-view-hierarchies\",\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# RICO Dataset - With Semantic Annotations\n",
    "rico_dataset2 = load_dataset(\n",
    "    path=\"shunk031/Rico\",\n",
    "    name=\"ui-screenshots-and-hierarchies-with-semantic-annotations\",\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# MobileViews Dataset - Screenshots and View Hierarchies\n",
    "mobileviews_dataset = load_dataset(\n",
    "    path=\"mllmTeam/MobileViews\",\n",
    "    data_dir=\"MobileViews_Screenshots_ViewHierarchies/Parquets\",\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# Combine all three datasets\n",
    "combined_dataset = itertools.chain(rico_dataset1, rico_dataset2, mobileviews_dataset)\n",
    "\n",
    "print(\"✅ Loaded datasets: RICO (2 splits) + MobileViews\")\n",
    "\n",
    "# ================================\n",
    "# 3. Load tokenizer\n",
    "# ================================\n",
    "model_name = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# ================================\n",
    "# 4️. Load base model with 4-bit quant + GPU offload\n",
    "# ================================\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Memory limits for g5 EC2 instance (32GB RAM, ~24GB GPU)\n",
    "# Adjusted for 14B model + training overhead\n",
    "max_memory = {\n",
    "    \"cpu\": \"28GB\",      # Leave 4GB for system\n",
    "    0: \"22GB\"           # GPU 0 - g5.xlarge has 24GB VRAM, leave 2GB buffer\n",
    "}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    max_memory=max_memory,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# 5️. LoRA configuration (optimized for 14B model)\n",
    "# ================================\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                                    # Increased from 8 for larger model\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # More modules for better adaptation\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# ================================\n",
    "# 6️. Memory-efficient IterableDataset\n",
    "# ================================\n",
    "class StreamingRicoDataset(IterableDataset):\n",
    "    def __init__(self, dataset_iter, tokenizer, max_length=512):\n",
    "        self.dataset_iter = dataset_iter\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __iter__(self):\n",
    "        for example in self.dataset_iter:\n",
    "            # Convert all fields to a single string safely\n",
    "            parts = [f\"{k}: {str(v)}\" for k, v in example.items() if v is not None]\n",
    "            text = \" | \".join(parts)\n",
    "            tokenized = self.tokenizer(\n",
    "                text, truncation=True, padding=\"max_length\",\n",
    "                max_length=self.max_length, return_tensors=\"pt\"\n",
    "            )\n",
    "            yield {\n",
    "                \"input_ids\": tokenized[\"input_ids\"].squeeze(0),\n",
    "                \"attention_mask\": tokenized[\"attention_mask\"].squeeze(0),\n",
    "                \"labels\": tokenized[\"input_ids\"].squeeze(0)  # causal LM\n",
    "            }\n",
    "\n",
    "train_dataset = StreamingRicoDataset(combined_dataset, tokenizer)\n",
    "\n",
    "# ================================\n",
    "# 7️. Data collator\n",
    "# ================================\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "# ================================\n",
    "# 8️. Training arguments (optimized for g5 EC2)\n",
    "# ================================\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen2.5_14b_rico_mobileviews_lora\",\n",
    "    per_device_train_batch_size=2,        # Increased from 1 (more GPU memory)\n",
    "    gradient_accumulation_steps=8,        # Reduced from 16 (effective batch = 16)\n",
    "    warmup_steps=100,                     # Increased warmup\n",
    "    max_steps=1500,                       # More steps for additional dataset\n",
    "    learning_rate=2e-4,                   # Slightly lower for 14B model\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_steps=250,                       # Save checkpoints\n",
    "    save_total_limit=3,                   # Keep more checkpoints\n",
    "    gradient_checkpointing=True,          # Enable for memory efficiency\n",
    "    optim=\"paged_adamw_8bit\",            # Memory-efficient optimizer\n",
    "    dataloader_num_workers=4,             # Utilize CPU cores\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# 9️. Trainer\n",
    "# ================================\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# 10. Empty CUDA cache before training\n",
    "# ================================\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ================================\n",
    "# 11. Fine-tune\n",
    "# ================================\n",
    "trainer.train()\n",
    "\n",
    "# ================================\n",
    "# 1️2. Save LoRA adapters + tokenizer\n",
    "# ================================\n",
    "model.save_pretrained(\"./qwen2.5_14b_rico_mobileviews_lora\")\n",
    "tokenizer.save_pretrained(\"./qwen2.5_14b_rico_mobileviews_lora\")\n",
    "print(\"Fine-tuning complete! LoRA adapters saved to ./qwen2.5_14b_rico_mobileviews_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dedd809",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Optional: Compress and download adapters (for EC2 to local transfer)\n",
    "# ================================\n",
    "# Uncomment if you want to compress the output folder\n",
    "# !tar -czf qwen2.5_14b_rico_mobileviews_lora.tar.gz qwen2.5_14b_rico_mobileviews_lora\n",
    "\n",
    "# For EC2, you can use SCP or AWS S3 to transfer files:\n",
    "# Example S3 upload:\n",
    "# !aws s3 cp qwen2.5_14b_rico_mobileviews_lora s3://your-bucket/models/ --recursive\n",
    "\n",
    "# Example SCP to local machine:\n",
    "# scp -i your-key.pem -r ubuntu@ec2-instance:/path/to/qwen2.5_14b_rico_mobileviews_lora ./local-folder/\n",
    "\n",
    "print(\"Model saved locally at: ./qwen2.5_14b_rico_mobileviews_lora\")\n",
    "print(\"Use SCP, S3, or download directly from EC2 to transfer the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c6e212",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
